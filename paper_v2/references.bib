@INPROCEEDINGS{data_paper,
  author={Zhu, Xuanying and Gedeon, Tom and Caldwell, Sabrina and Jones, Richard},
  booktitle={2019 IEEE 23rd International Conference on Intelligent Engineering Systems (INES)}, 
  title={Detecting emotional reactions to videos of depression}, 
  year={2019},
  volume={},
  number={},
  pages={000147-000152},
  doi={10.1109/INES46365.2019.9109519}}

@InProceedings{casper_paper,
author="Treadgold, N. K.
and Gedeon, T. D.",
editor="Mira, Jos{\'e}
and Moreno-D{\'i}az, Roberto
and Cabestany, Joan",
title="A cascade network algorithm employing Progressive RPROP",
booktitle="Biological and Artificial Computation: From Neuroscience to Technology",
year="1997",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="733--742",
abstract="Cascade Correlation (Cascor) has proved to be a powerful method for training neural networks. Cascor, however, has been shown not to generalise well on regression and some classification problems. A new Cascade network algorithm employing Progressive RPROP (Casper), is proposed. Casper, like Cascor, is a constructive learning algorithm which builds cascade networks. Instead of using weight freezing and a correlation measure to install new neurons, however, Casper uses a variation of RPROP to train the whole network. Casper is shown to produce more compact networks, which generalise better than Cascor.",
isbn="978-3-540-69074-0"
}

@inproceedings{deq_paper,
  author    = {Shaojie Bai and J. Zico Kolter and Vladlen Koltun},
  title     = {Deep Equilibrium Models},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2019},
}

@article{x_ray1_Bassi_2021,
   title={A deep convolutional neural network for COVID-19 detection using chest X-rays},
   ISSN={2446-4740},
   url={http://dx.doi.org/10.1007/s42600-021-00132-9},
   DOI={10.1007/s42600-021-00132-9},
   journal={Research on Biomedical Engineering},
   publisher={Springer Science and Business Media LLC},
   author={Bassi, Pedro R. A. S. and Attux, Romis},
   year={2021},
   month={Apr}
}
@article{x_ray2_SAHA2021100505,
title = {EMCNet: Automated COVID-19 diagnosis from X-ray images using convolutional neural network and ensemble of machine learning classifiers},
journal = {Informatics in Medicine Unlocked},
volume = {22},
pages = {100505},
year = {2021},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2020.100505},
url = {https://www.sciencedirect.com/science/article/pii/S2352914820306560},
author = {Prottoy Saha and Muhammad Sheikh Sadi and Md. Milon Islam},
keywords = {COVID-19, Convolutional neural network, Ensemble of classifiers, Automatic diagnosis, X-ray images},
abstract = {Recently, coronavirus disease (COVID-19) has caused a serious effect on the healthcare system and the overall global economy. Doctors, researchers, and experts are focusing on alternative ways for the rapid detection of COVID-19, such as the development of automatic COVID-19 detection systems. In this paper, an automated detection scheme named EMCNet was proposed to identify COVID-19 patients by evaluating chest X-ray images. A convolutional neural network was developed focusing on the simplicity of the model to extract deep and high-level features from X-ray images of patients infected with COVID-19. With the extracted features, binary machine learning classifiers (random forest, support vector machine, decision tree, and AdaBoost) were developed for the detection of COVID-19. Finally, these classifiers’ outputs were combined to develop an ensemble of classifiers, which ensures better results for the dataset of various sizes and resolutions. In comparison with other recent deep learning-based systems, EMCNet showed better performance with 98.91% accuracy, 100% precision, 97.82% recall, and 98.89% F1-score. The system could maintain its great importance on the automatic detection of COVID-19 through instant detection and low false negative rate.}
}

@article{Rajkomar2018,
author={Rajkomar, Alvin and Oren, Eyal and Chen, Kai
and Dai, Andrew M.
and Hajaj, Nissan
and Hardt, Michaela
and Liu, Peter J.
and Liu, Xiaobing
and Marcus, Jake
and Sun, Mimi
and Sundberg, Patrik
and Yee, Hector
and Zhang, Kun
and Zhang, Yi
and Flores, Gerardo
and Duggan, Gavin E.
and Irvine, Jamie
and Le, Quoc
and Litsch, Kurt
and Mossin, Alexander
and Tansuwan, Justin
and Wang, De
and Wexler, James
and Wilson, Jimbo
and Ludwig, Dana
and Volchenboum, Samuel L.
and Chou, Katherine
and Pearson, Michael
and Madabushi, Srinivasan
and Shah, Nigam H.
and Butte, Atul J.
and Howell, Michael D.
and Cui, Claire
and Corrado, Greg S.
and Dean, Jeffrey},
title={Scalable and accurate deep learning with electronic health records},
journal={npj Digital Medicine},
year={2018},
month={May},
day={08},
volume={1},
number={1},
pages={18},
abstract={Predictive modeling with electronic health record (EHR) data is anticipated to drive personalized medicine and improve healthcare quality. Constructing predictive statistical models typically requires extraction of curated predictor variables from normalized EHR data, a labor-intensive process that discards the vast majority of information in each patient's record. We propose a representation of patients' entire raw EHR records based on the Fast Healthcare Interoperability Resources (FHIR) format. We demonstrate that deep learning methods using this representation are capable of accurately predicting multiple medical events from multiple centers without site-specific data harmonization. We validated our approach using de-identified EHR data from two US academic medical centers with 216,221 adult patients hospitalized for at least 24{\thinspace}h. In the sequential format we propose, this volume of EHR data unrolled into a total of 46,864,534,945 data points, including clinical notes. Deep learning models achieved high accuracy for tasks such as predicting: in-hospital mortality (area under the receiver operator curve [AUROC] across sites 0.93--0.94), 30-day unplanned readmission (AUROC 0.75--0.76), prolonged length of stay (AUROC 0.85--0.86), and all of a patient's final discharge diagnoses (frequency-weighted AUROC 0.90). These models outperformed traditional, clinically-used predictive models in all cases. We believe that this approach can be used to create accurate and scalable predictions for a variety of clinical scenarios. In a case study of a particular prediction, we demonstrate that neural networks can be used to identify relevant information from the patient's chart.},
issn={2398-6352},
doi={10.1038/s41746-018-0029-1},
url={https://doi.org/10.1038/s41746-018-0029-1}
}

 @Article{Esteva2017,
author={Esteva, Andre
and Kuprel, Brett
and Novoa, Roberto A.
and Ko, Justin
and Swetter, Susan M.
and Blau, Helen M.
and Thrun, Sebastian},
title={Dermatologist-level classification of skin cancer with deep neural networks},
journal={Nature},
year={2017},
month={Feb},
day={01},
volume={542},
number={7639},
pages={115-118},
abstract={An artificial intelligence trained to classify images of skin lesions as benign lesions or malignant skin cancers achieves the accuracy of board-certified dermatologists.},
issn={1476-4687},
doi={10.1038/nature21056},
url={https://doi.org/10.1038/nature21056}
}

@inbook{cascor_paper,
author = {Fahlman, Scott E. and Lebiere, Christian},
title = {The Cascade-Correlation Learning Architecture},
year = {1990},
isbn = {1558601007},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Advances in Neural Information Processing Systems 2},
pages = {524–532},
numpages = {9}
}

@unpublished{rmsprop,
author = {Tijmen, Tieleman and Geoffrey, Hinton},
title = {Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
note = {cousera: Neural Networks for Machine Learning},
year = {2012},
month = {10},
page = {26-31},
volume={4},
issue={2}
}

@misc{neural_ode,
      title={Neural Ordinary Differential Equations}, 
      author={Ricky T. Q. Chen and Yulia Rubanova and Jesse Bettencourt and David Duvenaud},
      year={2019},
      eprint={1806.07366},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ddn,
      title={Deep Declarative Networks: A New Hope}, 
      author={Stephen Gould and Richard Hartley and Dylan Campbell},
      year={2020},
      eprint={1909.04866},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@Inbook{LeCun2012,
author="LeCun, Yann A.
and Bottou, L{\'e}on
and Orr, Genevieve B.
and M{\"u}ller, Klaus-Robert",
editor="Montavon, Gr{\'e}goire
and Orr, Genevi{\`e}ve B.
and M{\"u}ller, Klaus-Robert",
title="Efficient BackProp",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="9--48",
abstract="The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.",
isbn="978-3-642-35289-8",
doi="10.1007/978-3-642-35289-8\_3",
url="https://doi.org/10.1007/978-3-642-35289-8\_3"
}

@inproceedings{NIPS2011_e836d813,
 author = {Wiesler, Simon and Ney, Hermann},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Convergence Analysis of Log-Linear Training},
 url = {https://proceedings.neurips.cc/paper/2011/file/e836d813fd184325132fca8edcdfb40e-Paper.pdf},
 volume = {24},
 year = {2011}
}

@unpublished{cascade_lecture,
author = {Tom, Gedeon and Nathan, Elazar},
title = {Lecture NN9: Cascade networks},
note = {The ANU: COMP8420 - Neural Networks, Deep Learning and Bio-inspired Computing},
year = {2021}
}

@unpublished{implicit_tutorial,
author = {David, Duvenaud and Zico, Kolter and Matt Johnson},
title = {Tutorial: Deep Implicit Layers: Neural ODEs, Equilibrium Models, and Beyond},
note = {Conference and Workshop on Neural Information Processing Systems 2020},
year = {2020}
}

@misc{bai2019trellis,
      title={Trellis Networks for Sequence Modeling}, 
      author={Shaojie Bai and J. Zico Kolter and Vladlen Koltun},
      year={2019},
      eprint={1810.06682},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@article{anderson,
author = {Walker, Homer F. and Ni, Peng},
title = {Anderson Acceleration for Fixed-Point Iterations},
year = {2011},
issue_date = {July 2011},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {49},
number = {4},
issn = {0036-1429},
url = {https://doi.org/10.1137/10078356X},
doi = {10.1137/10078356X},
abstract = {This paper concerns an acceleration method for fixed-point iterations that originated in work of D. G. Anderson [J. Assoc. Comput. Mach., 12 (1965), pp. 547-560], which we accordingly call Anderson acceleration here. This method has enjoyed considerable success and wide usage in electronic structure computations, where it is known as Anderson mixing; however, it seems to have been untried or underexploited in many other important applications. Moreover, while other acceleration methods have been extensively studied by the mathematics and numerical analysis communities, this method has received relatively little attention from these communities over the years. A recent paper by H. Fang and Y. Saad [Numer. Linear Algebra Appl., 16 (2009), pp. 197-221] has clarified a remarkable relationship of Anderson acceleration to quasi-Newton (secant updating) methods and extended it to define a broader Anderson family of acceleration methods. In this paper, our goals are to shed additional light on Anderson acceleration and to draw further attention to its usefulness as a general tool. We first show that, on linear problems, Anderson acceleration without truncation is “essentially equivalent” in a certain sense to the generalized minimal residual (GMRES) method. We also show that the Type 1 variant in the Fang-Saad Anderson family is similarly essentially equivalent to the Arnoldi (full orthogonalization) method. We then discuss practical considerations for implementing Anderson acceleration and illustrate its performance through numerical experiments involving a variety of applications.},
journal = {SIAM J. Numer. Anal.},
month = aug,
pages = {1715–1735},
numpages = {21},
keywords = {nonnegative matrix factorization, Arnoldi (full orthogonalization) method, generalized minimal residual method, fixed-point iterations, alternating least-squares, mixture densities, expectation-maximization algorithm, acceleration methods, domain decomposition, iterative methods}
}