{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A1-3-Fixed_Point_Casper.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1Pr7rRavqd9T_e7MFd9HtqVhm74jOoH1K","authorship_tag":"ABX9TyODfzRw8Kwmom7VtI9jLvcS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e3a895df325243e69c96dba3a41ecc56":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7ae49325881e4c789949dbe98e408e87","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f555d8c5b6af46439f8d20f951c5e6e8","IPY_MODEL_98826df08237430f85848ff69c4ed4d7"]}},"7ae49325881e4c789949dbe98e408e87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f555d8c5b6af46439f8d20f951c5e6e8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_26080cfcb9fc48e1a9898a7784fe440f","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":12,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":12,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_499ed7b0766c429689686096d81a1a03"}},"98826df08237430f85848ff69c4ed4d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_364868ab47ae42b5acb7ba978a834957","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 12/12 [00:32&lt;00:00,  2.72s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d9ca3a3a462b4a99afde1c4515cdb35a"}},"26080cfcb9fc48e1a9898a7784fe440f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"499ed7b0766c429689686096d81a1a03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"364868ab47ae42b5acb7ba978a834957":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d9ca3a3a462b4a99afde1c4515cdb35a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TXeRhu0XFh7m","executionInfo":{"status":"ok","timestamp":1619422253971,"user_tz":-600,"elapsed":1280,"user":{"displayName":"Chính Lã","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFHHlWScmgagOtnIwLdvwL89Ym2g4gSHVUopZn=s64","userId":"10710531293520746775"}},"outputId":"22b72caf-cc0b-457e-f8c6-ce65f1dac8c7"},"source":["# %cd drive/MyDrive/Colab\\ Notebooks/COMP8420"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/COMP8420\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-JEdQ7OSFnoH"},"source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from pandas import DataFrame"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FoLai-ufFoc9"},"source":["import os\n","from tqdm.notebook import tqdm\n","from typing import List, Tuple, Dict\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HHDwCxMZFwkj"},"source":["## Data related function"]},{"cell_type":"code","metadata":{"id":"30bTqMzTFqGC"},"source":["def load_data(path:str, mode:str)-> DataFrame:\n","    \"\"\"This function load data from specific data file based on mode\n","\n","    Args:\n","        path (str): path to folder contains data file\n","        mode (str): what data to be loaded\n","\n","    Returns:\n","        DataFrame: DataFrame contains all data\n","    \"\"\"\n","    assert mode in [\"gsr\", \"pupil\", \"skin\", \"all\"], f\"mode should be 'gsr','pupil','all'. Not {mode}\"\n","    # read data gsr\n","    if mode in ['gsr', 'all']:\n","        df_gsr = pd.read_excel(path+'gsr_features.xlsx',sheet_name='data')\n","        df_gsr.columns = ['patient'] + list(df_gsr.columns)[1:]\n","        if mode == 'gsr':\n","            return df_gsr\n","    # read pupil data\n","    if mode in ['pupil', 'all']:\n","        df_pupil = pd.read_excel(path+'pupil_features.xlsx',sheet_name='data')\n","        df_pupil.columns = ['patient'] + list(df_pupil.columns)[1:]\n","        if mode == 'pupil':\n","            return df_pupil\n","    # read skin temperature data\n","    if mode in ['skin', 'all']:\n","        df_skin = pd.read_excel(path+'skintemp_features.xlsx',sheet_name='data')\n","        df_skin.columns = ['patient'] + list(df_skin.columns)[1:]\n","        if mode == 'skin':\n","            return df_skin\n","    # in case of using all data, combine\n","    data = pd.concat([df_gsr,df_pupil,df_skin],axis = 1)\n","    # remove duplicated columns\n","    data = data.loc[:,~data.columns.duplicated()]\n","    return data\n","\n","def train_val_test_split(data:DataFrame, test_patient:str, val_patient:str) -> Tuple[DataFrame,DataFrame,DataFrame]:\n","    \"\"\" This function is to create train, test and validation dataset using leave-one-patient-out\n","\n","    Args:\n","        data (DataFrame): the dataset that contains all data point\n","        test_patient (str): patient choosen for testing\n","        val_patient (str): patient choosen for validation in training process\n","\n","    Returns:\n","        Tuple[DataFrame,DataFrame,DataFrame]: train, validation, test dataframe\n","    \"\"\"\n","    # check input\n","    assert test_patient in data.patient.unique(), f'{data.patient.unique().tolist()}'\n","    assert val_patient in data.patient.unique(), f'{data.patient.unique().tolist()}'\n","    # create train and test dataset\n","    df_test = data[data['patient'] == test_patient]\n","    df_train = data[(data['patient'] != test_patient)&((data['patient'] != val_patient))]\n","    df_val = data[data['patient'] == val_patient]\n","    # reset index\n","    df_test = df_test.reset_index(drop=True)\n","    df_train = df_train.reset_index(drop=True)\n","    df_val = df_val.reset_index(drop = True)\n","    # delete patient column\n","    if 'patient' in df_train.columns:\n","        del df_train['patient']\n","    if 'patient' in df_test.columns:\n","        del df_test['patient']  \n","    if 'patient' in df_val.columns:\n","        del df_val['patient']\n","    return df_train, df_val, df_test\n","\n","\n","def df_to_xy(df:DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n","    \"\"\"Convert dataframe to X and Y numpy array where X is input and Y is output\n","\n","    Args:\n","        df (DataFrame): dataset to convert to numpy array\n","\n","    Returns:\n","        Tuple[np.ndarray, np.ndarray]: X,Y\n","    \"\"\"\n","    df = df.apply(pd.to_numeric)\n","    x = df.values[:,1:]\n","    y = df.values[:,0]\n","    return x,y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aG7wuhhrF3dC"},"source":["## Fixed Point Casper model"]},{"cell_type":"code","metadata":{"id":"qDIrnulOFzPd"},"source":["import torch.autograd as autograd\n","\n","class DEQFixedPoint(nn.Module):\n","    def __init__(self, f, solver, **kwargs):\n","        super().__init__()\n","        self.f = f\n","        self.solver = solver\n","        self.kwargs = kwargs\n","        \n","    def forward(self, x):\n","        # compute forward pass and re-engage autograd tape\n","        with torch.no_grad():\n","            z, self.forward_res = self.solver(lambda z : self.f(z, x), torch.zeros((x.shape[0],1)), **self.kwargs)\n","        z = self.f(z,x)\n","        \n","        # set up Jacobian vector product (without additional forward calls)\n","        z0 = z.clone().detach().requires_grad_()\n","        f0 = self.f(z0,x)\n","        def backward_hook(grad):\n","            g, self.backward_res = self.solver(lambda y : autograd.grad(f0, z0, y, retain_graph=True)[0] + grad,\n","                                               grad, **self.kwargs)\n","            return g\n","                \n","        z.register_hook(backward_hook)\n","        return z\n","\n","class CascadeNeuron(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super().__init__()\n","        self.fc1 = nn.Linear(input_size, output_size,bias = False)\n","        self.fc2 = nn.Linear(output_size, output_size, bias = False)\n","\n","    def forward(self, z, x):\n","        # print(x)\n","        x1 = self.fc1(x)\n","        # print(z)\n","        x2 = self.fc2(z)\n","        out = torch.sigmoid(x1+x2)\n","        return out\n","\n","\n","def anderson(f, x0, m=5, lam=1e-4, max_iter=50, tol=1e-2, beta = 1.0):\n","    \"\"\" Anderson acceleration for fixed point iteration. \"\"\"\n","    bsz, d = x0.shape\n","    X = torch.zeros(bsz, m, d, dtype=x0.dtype, device=x0.device)\n","    F = torch.zeros(bsz, m, d, dtype=x0.dtype, device=x0.device)\n","    X[:,0] = x0.view(bsz, -1)\n","    \n","    F[:,0] = f(x0).view(bsz, -1)\n","    X[:,1], F[:,1] = F[:,0], f(F[:,0].view_as(x0)).view(bsz, -1)\n","    \n","    H = torch.zeros(bsz, m+1, m+1, dtype=x0.dtype, device=x0.device)\n","    H[:,0,1:] = H[:,1:,0] = 1\n","    y = torch.zeros(bsz, m+1, 1, dtype=x0.dtype, device=x0.device)\n","    y[:,0] = 1\n","    \n","    res = []\n","    for k in range(2, max_iter):\n","        n = min(k, m)\n","        G = F[:,:n]-X[:,:n]\n","        H[:,1:n+1,1:n+1] = torch.bmm(G,G.transpose(1,2)) + lam*torch.eye(n, dtype=x0.dtype,device=x0.device)[None]\n","        alpha = torch.solve(y[:,:n+1], H[:,:n+1,:n+1])[0][:, 1:n+1, 0]   # (bsz x n)\n","        \n","        X[:,k%m] = beta * (alpha[:,None] @ F[:,:n])[:,0] + (1-beta)*(alpha[:,None] @ X[:,:n])[:,0]\n","        F[:,k%m] = f(X[:,k%m].view_as(x0)).view(bsz, -1)\n","        res.append((F[:,k%m] - X[:,k%m]).norm().item()/(1e-5 + F[:,k%m].norm().item()))\n","        if (res[-1] < tol):\n","            break\n","    return X[:,k%m].view_as(x0), res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M6PhcqrQF-ZF"},"source":["class CasPer(nn.Module):\n","    def __init__(self, input_size, num_classes, input_hidden_layers, hidden_hidden_layers, hidden_output_layers):\n","        super().__init__()\n","        self.n_hidden_layers = 0\n","        self.fc1 = nn.Linear(input_size, num_classes)\n","\n","        self.input_size = input_size\n","        self.num_classes = num_classes\n","\n","        # Init number of n*(n+3)/2 layers for use\n","        self.input_hidden_layers = input_hidden_layers\n","        # self.hidden_hidden_layers = hidden_hidden_layers\n","        self.hidden_output_layers = hidden_output_layers\n","\n","    def forward(self, x):\n","        outL3_1 = self.fc1(x)  # part of L3 weights, correlation of input and output\n","        if self.n_hidden_layers == 0:\n","            return outL3_1\n","        # when there exists a hidden neuron\n","        H = []                  # store hidden neuron value\n","        x_clone = x.clone()     # extend neuraon value to the end of this\n","        for i in range(self.n_hidden_layers):\n","            # print(x_clone.shape)\n","            # calculate first neuron\n","            if i == 0:\n","                h = self.input_hidden_layers[str(i)](x_clone)\n","                h = torch.sigmoid(h)\n","                H.append(h)\n","                continue\n","            # calculate other neuron value\n","            previous_neuron = H[i-1]\n","            # print(previous_neuron.shape)\n","            # ones = torch.tensor((x.shape[0],1),dtype=torch.float)\n","            x_clone = torch.cat([x_clone, previous_neuron], dim=1)\n","            # calculate next neuron value\n","            h = self.input_hidden_layers[str(i)](x_clone)\n","            h = torch.sigmoid(h)\n","            H.append(h)\n","        # calculate output\n","        total_out = outL3_1\n","        for h in H:\n","            total_out += self.hidden_output_layers[str(i)](h)\n","        return total_out\n","    \n","    def add_neuron(self, learning_rates = (0.2, 0.005, 0.001), deq = False):\n","        # save total number of neuron\n","        self.n_hidden_layers += 1\n","        if deq:\n","            # edit DEQ\n","            f = CascadeNeuron(self.input_size + self.n_hidden_layers - 1, 1)\n","            # add new connection from input to new neuron\n","            self.input_hidden_layers[str(len(self.input_hidden_layers))] = DEQFixedPoint(f,anderson)\n","        else:\n","             # add new connection from input to new neuron\n","            self.input_hidden_layers[str(len(self.input_hidden_layers))] = nn.Linear(self.input_size + self.n_hidden_layers - 1, 1, bias = False)\n","        # add new connections from new neuron to output\n","        self.hidden_output_layers[str(len(self.hidden_output_layers))] = nn.Linear(1, self.num_classes, bias = False)\n","        # create new optimizer\n","        # L1 is for connection from input to new neuron\n","        # from input to new neuron\n","        L1_params_id = list(map(id, self.input_hidden_layers[str(len(input_hidden_layers)-1)].parameters()))\n","        # L2 is for connection from new neuron to output\n","        L2_params_id = list(map(id, self.hidden_output_layers[str(len(self.hidden_output_layers)-1)].parameters()))\n","        L1L2_id = L1_params_id + L2_params_id\n","        # L3 is everything else not in L1, L2 region\n","        L3_params = filter(lambda p: id(p) not in L1L2_id, self.parameters())\n","        # get learning rate for each region\n","        lr1, lr2, lr3 = learning_rates\n","        \n","        params = [\n","            {'params': L3_params, 'lr':lr3}, # L3\n","            {'params': self.input_hidden_layers[str(len(input_hidden_layers)-1)].parameters(), 'lr':lr1}, # L1\n","            {'params': self.hidden_output_layers[str(len(self.hidden_output_layers)-1)].parameters(), 'lr':lr2}, # L2\n","        ]      \n","        optimizer = torch.optim.RMSprop(params, momentum=0.9, weight_decay=0.00001, centered=True)\n","        # optimizer = optim.Adam(params)\n","        return optimizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m1RV_XZ6GCf1"},"source":["## Ultities"]},{"cell_type":"code","metadata":{"id":"RYzB9oApGBYF"},"source":["def accuracy(Y_pred, Y_target):\n","    \"\"\" Calculate accuracy of prediction (Y_pred) compared to truth label (Y_target)\n","    Args:\n","        Y_pred (torch.tensor): prediction tensor with size of [batch x num_class]\n","        Y_target (torch.tensor): truth tensor with size of [batch x 1]\n","    Returns:\n","        torch.float: the overall accuracy\n","    \"\"\"\n","    # find number of correct label\n","    correct_num = (Y_pred.argmax(-1) == Y_target).int().sum()\n","    # calculate accuracy\n","    acc = correct_num.float()/Y_target.shape[0]\n","    return acc\n","\n","\n","def recall(Y_pred, Y_target, class_label):\n","    \"\"\" Calculate Recall\n","    Recall = TP/(TP+FN)\n","    Args:\n","        Y_pred (torch.tensor): prediction tensor with size of [batch x num_class]\n","        Y_target (torch.tensor): truth tensor with size of [batch x 1]\n","        claass_label (str): label of class that we want to calculate recall\n","    Returns:\n","        torch.float: recall of model\n","    \"\"\"\n","    # get label prediction\n","    Y_pred = Y_pred.argmax(-1)\n","    # find all datapoint that is true for the class_label\n","    label_index = torch.where(Y_target == class_label)\n","    # find all correctly predicted label in this class\n","    correct_label = (Y_pred[label_index] == Y_target[label_index]).int().sum()\n","    # returnr recall\n","    return float(correct_label)/len(label_index[0])\n","\n","\n","def precision(Y_pred, Y_target, class_label):\n","    \"\"\" Calculate Precision\n","    Recall = TP/(TP+FP)\n","    Args:\n","        Y_pred (torch.tensor): prediction tensor with size of [batch x num_class]\n","        Y_target (torch.tensor): truth tensor with size of [batch x 1]\n","        claass_label (str): label of class that we want to calculate precision\n","    Returns:\n","        torch.float: precision of model\n","    \"\"\"\n","    # get label prediction\n","    Y_pred = Y_pred.argmax(-1)\n","    # get all correct label index\n","    label_index = torch.where(Y_target == class_label)\n","    # get all correctly predicted label for the class label\n","    correct_label = (Y_pred[label_index] == Y_target[label_index]).int().sum()\n","    # return precision\n","    return float(correct_label)/max(len(torch.where(Y_pred == class_label)[0]),1)\n","\n","\n","def f1(recall, precision):\n","    \"\"\" Calculate F1 score\n","    F1 =  2 * precision * recall/(precision + recall)\n","    Args:\n","        recall (float): recall value\n","        precision (float): precision value\n","    Returns:\n","        float: f1 score\n","    \"\"\"\n","    return 2 * recall * precision / max((recall + precision),1)\n","\n","\n","def validation(model, criterion, X_val, Y_val, test = False):\n","    \"\"\" Calculate all metrics of trained model\n","    Args:\n","        model (nn.Module): the model we need to evaluate\n","        criterion (nn.Module): loss function\n","        X_val (torch.tensor): input tensor used for validation with size of [batch x dimension]\n","        Y_val (torch.tensor): output tensor used for validation with size of [batch x 1]\n","        test (bool): set this to print output to debug/compare\n","    Returns:\n","        loss: loss function value\n","        acc: accuracy of model\n","        rc_all: recall for each class\n","        pr_all: precision for each class\n","        f1_all: F1 score for each class\n","    \"\"\"\n","    # set model to evaluation mode\n","    model.eval()\n","    # calculate output\n","    outputs = model(X_val)\n","    # calculate loss_function\n","    loss = criterion(outputs, Y_val)\n","    # calculate accuracy\n","    acc = accuracy(outputs, Y_val)\n","\n","    rc_all = []\n","    pr_all = []\n","    f1_all = []\n","    for i in range(4):\n","        # recall\n","        rc = recall(outputs, Y_val, i)\n","        # precision\n","        pr = precision(outputs, Y_val, i)\n","        # f1\n","        f1_value = f1(rc,pr)\n","        # Store result\n","        rc_all.append(rc)\n","        pr_all.append(pr)\n","        f1_all.append(f1_value)\n","    if test:\n","        print('Predict\\t',outputs.argmax(-1))\n","        print('Test\\t',Y_val)\n","    return loss, acc, rc_all, pr_all, f1_all"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u994S0G0HoD7"},"source":["## Training and Validating"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":729,"referenced_widgets":["e3a895df325243e69c96dba3a41ecc56","7ae49325881e4c789949dbe98e408e87","f555d8c5b6af46439f8d20f951c5e6e8","98826df08237430f85848ff69c4ed4d7","26080cfcb9fc48e1a9898a7784fe440f","499ed7b0766c429689686096d81a1a03","364868ab47ae42b5acb7ba978a834957","d9ca3a3a462b4a99afde1c4515cdb35a"]},"id":"6B6FTuviGKvm","executionInfo":{"status":"ok","timestamp":1619422540672,"user_tz":-600,"elapsed":35632,"user":{"displayName":"Chính Lã","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFHHlWScmgagOtnIwLdvwL89Ym2g4gSHVUopZn=s64","userId":"10710531293520746775"}},"outputId":"ec8d960b-6039-4a20-c179-172d3e6c915f"},"source":["# set this to true to use fixed point layer instead of normal layer\n","deq_layer = True\n","max_layer = 2\n","# 12321\n","torch.manual_seed(12321)\n","# 268\n","np.random.seed(268)\n","# data folder\n","root = './data/'\n","# load data set\n","dataset = load_data(root, 'all')\n","# get all the patient to perform leave one patient out\n","patient_list = dataset.patient.unique().tolist()\n","# store metrics data\n","all_acc = []\n","all_recall = []\n","all_precision = []\n","all_f1 = []\n","\n","for patient in tqdm(patient_list):\n","    print(f'Choose patiet {patient} as testing patient')\n","    best_accuracy = 0\n","    best_val_p = None\n","    for val_p in patient_list:\n","\n","        # get train, val, test data\n","        train, val, test = train_val_test_split(dataset, patient,val_p)\n","        # create x,y\n","        train_x, train_y = df_to_xy(train)\n","        val_x, val_y = df_to_xy(val)\n","        test_x, test_y = df_to_xy(test)\n","        # calculate normalize parameter\n","        mean_train_x = train_x.mean(axis=0)\n","        std_train_x = train_x.std(axis=0)\n","        # normalize data\n","        normalize_train_x = (train_x - mean_train_x)/std_train_x\n","        normalize_val_x = (val_x - mean_train_x)/std_train_x\n","        normalize_test_x = (test_x - mean_train_x)/std_train_x\n","        # convert to tensor\n","        X_train = torch.tensor(normalize_train_x, dtype=torch.float)\n","        Y_train = torch.tensor(train_y, dtype=torch.long)\n","\n","        X_val = torch.tensor(normalize_val_x, dtype=torch.float)\n","        Y_val = torch.tensor(val_y, dtype=torch.long)\n","\n","        input_size = 85\n","        output_size = 4\n","\n","        input_hidden_layers = nn.ModuleDict()\n","        hidden_hidden_layers = nn.ModuleDict()\n","        hidden_output_layers = nn.ModuleDict()\n","\n","        # model = CasPer(input_size, output_size, input_hidden_layers, hidden_hidden_layers, hidden_output_layers)\n","        model = CasPer(input_size, output_size, input_hidden_layers, hidden_hidden_layers, hidden_output_layers)\n","        model.add_neuron(deq = deq_layer)\n","        criterion = nn.CrossEntropyLoss()\n","\n","        optimizer = optim.RMSprop(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.00001, centered=True)\n","        # optimizer = optim.Adam(model.parameters())\n","\n","        P = 2\n","        checkpoint_epoch = 15 + P * model.n_hidden_layers\n","\n","        previous_loss = torch.tensor(float('inf'))\n","        all_loss = []\n","\n","        num_epoch = 10000\n","        count = 0\n","        for epoch in range(num_epoch * 2):\n","            # if epoch %1000 == 0:\n","            #     clear_output()\n","            model.train()\n","            total = 0\n","            correct = 0\n","            optimizer.zero_grad()\n","            outputs = model(X_train)\n","            loss = criterion(outputs, Y_train)\n","            all_loss.append(loss)\n","            if epoch == checkpoint_epoch:\n","                N = model.n_hidden_layers\n","                # when the loss not decrease by 1%, add new neron\n","                if (previous_loss -loss).abs()/previous_loss < 0.005:\n","                    if N >= max_layer:\n","                        break\n","                    optimizer = model.add_neuron(deq = deq_layer)\n","                    N = model.n_hidden_layers\n","                    if checkpoint_epoch + (15+P*N) > num_epoch:\n","                        num_epoch += (15+P*N)\n","                # print(previous_loss,loss)\n","                previous_loss = loss\n","                # train\n","                Y_pred = outputs.argmax(-1)\n","                # print('Treain accuracy',sum((Y_pred == Y_train).int())/len(Y_train))\n","                checkpoint_epoch += 15 + P * N\n","        # validation\n","        val_loss, val_acc, rc_all, pr_all, f1_all = validation(model, criterion, X_val,Y_val,False)\n","        # save best model\n","        if val_acc > best_accuracy:\n","            best_accuracy = val_acc\n","            torch.save(model.state_dict(), './best_model/{model_name}_{patient}.pth'.format(model_name=type(model).__name__, patient=patient))\n","            best_p_val = val_p\n","\n","    # load best model to calculate accuracy\n","    model.load_state_dict(torch.load('./best_model/{model_name}_{patient}.pth'.format(model_name=type(model).__name__, patient=patient)))\n","    model.eval()\n","    # get train, val, test data\n","    train, val, test = train_val_test_split(dataset, patient,best_p_val)\n","    # create x,y\n","    train_x, train_y = df_to_xy(train)\n","    test_x, test_y = df_to_xy(test)\n","    # calculate normalize parameter\n","    mean_train_x = train_x.mean(axis=0)\n","    std_train_x = train_x.std(axis=0)\n","    # normalize data\n","    normalize_test_x = (test_x - mean_train_x)/std_train_x\n","    # convert to tensor\n","    X_test = torch.tensor(normalize_test_x, dtype=torch.float)\n","    Y_test = torch.tensor(test_y, dtype=torch.long)\n","\n","    test_loss, test_acc, rc_all, pr_all, f1_all = validation(model, criterion, X_test,Y_test, True)\n","    all_acc.append(test_acc)\n","    all_recall.append(rc_all)\n","    all_precision.append(pr_all)\n","    all_f1.append(f1_all)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e3a895df325243e69c96dba3a41ecc56","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Choose patiet p02 as testing patient\n","Predict\t tensor([3, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3])\n","Test\t tensor([2, 2, 2, 2, 0, 0, 0, 0, 3, 3, 3, 3, 1, 1, 1, 1])\n","Choose patiet p03 as testing patient\n","Predict\t tensor([3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 0])\n","Test\t tensor([1, 1, 1, 1, 3, 3, 3, 3, 0, 0, 0, 0, 2, 2, 2, 2])\n","Choose patiet p04 as testing patient\n","Predict\t tensor([0, 3, 0, 3, 0, 0, 2, 3, 2, 0, 2, 2, 2, 2, 0, 2])\n","Test\t tensor([3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0])\n","Choose patiet p06 as testing patient\n","Predict\t tensor([2, 0, 0, 0, 2, 3, 0, 0, 0, 0, 3, 0, 3, 3, 3, 3])\n","Test\t tensor([2, 2, 2, 2, 0, 0, 0, 0, 3, 3, 3, 3, 1, 1, 1, 1])\n","Choose patiet p07 as testing patient\n","Predict\t tensor([1, 1, 3, 1, 3, 3, 1, 1, 0, 3, 3, 3, 3, 3, 3, 1])\n","Test\t tensor([1, 1, 1, 1, 3, 3, 3, 3, 0, 0, 0, 0, 2, 2, 2, 2])\n","Choose patiet p08 as testing patient\n","Predict\t tensor([3, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n","Test\t tensor([3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0])\n","Choose patiet p09 as testing patient\n","Predict\t tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0])\n","Test\t tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3])\n","Choose patiet p10 as testing patient\n","Predict\t tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n","Test\t tensor([2, 2, 2, 2, 0, 0, 0, 0, 3, 3, 3, 3, 1, 1, 1, 1])\n","Choose patiet p11 as testing patient\n","Predict\t tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1])\n","Test\t tensor([1, 1, 1, 1, 3, 3, 3, 3, 0, 0, 0, 0, 2, 2, 2, 2])\n","Choose patiet p12 as testing patient\n","Predict\t tensor([0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","Test\t tensor([3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0])\n","Choose patiet p13 as testing patient\n","Predict\t tensor([1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1])\n","Test\t tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3])\n","Choose patiet p14 as testing patient\n","Predict\t tensor([1, 1, 1, 0, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1])\n","Test\t tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3])\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"00BQAjekGfOu","executionInfo":{"status":"ok","timestamp":1619422582424,"user_tz":-600,"elapsed":851,"user":{"displayName":"Chính Lã","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFHHlWScmgagOtnIwLdvwL89Ym2g4gSHVUopZn=s64","userId":"10710531293520746775"}},"outputId":"7ab9f9b1-e6a2-41ff-e5e8-ab4138c48ef2"},"source":["all_acc = np.array(all_acc)\n","all_recall = np.array(all_recall)\n","all_precision = np.array(all_precision)\n","all_f1 = np.array(all_f1)\n","print(\"==============================\")\n","print('Average accuracy:',all_acc.mean())\n","print('Recall:',all_recall.mean(axis=0))\n","print('Precision:',all_precision.mean(axis=0))\n","print('F1:',all_f1.mean(axis=0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["==============================\n","Average accuracy: 0.3125\n","Recall: [0.35416667 0.33333333 0.25       0.3125    ]\n","Precision: [0.29250611 0.17896825 0.31349206 0.24166667]\n","F1: [0.22625272 0.20873016 0.18978429 0.22753867]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BO_KrQQHGvy-","executionInfo":{"status":"ok","timestamp":1619422582906,"user_tz":-600,"elapsed":1008,"user":{"displayName":"Chính Lã","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFHHlWScmgagOtnIwLdvwL89Ym2g4gSHVUopZn=s64","userId":"10710531293520746775"}},"outputId":"6d5827bc-7926-4040-dc35-b70e555d5009"},"source":["print('Average Recall:',all_recall.mean(axis=0).mean())\n","print('Average Precision:',all_precision.mean(axis=0).mean())\n","print('Average F1:',all_f1.mean(axis=0).mean())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Average Recall: 0.3125\n","Average Precision: 0.25665827228327226\n","Average F1: 0.2130764592161651\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"abS6xRgjGzi2"},"source":[],"execution_count":null,"outputs":[]}]}